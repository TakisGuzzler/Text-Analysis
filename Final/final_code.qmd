---
title: "Final Report"
author: "Amalia, Lauren, and Jenny"
date: last-modified
format:
  pdf:
    number-sections: true
    indent: true
    toc: true
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
execute:
  echo: false
bibliography: citation.bib
nocite: |
  @the_other_side_of_the_hedge_1945,
  @the_old_nurses_story_1852,
  @the_old_man_at_the_bridge_1952,
  @the_mindworm_1950,
  @the_masque_of_the_red_death_1842,
  @the_man_who_killed_the_world_1940,
  @the_lifted_veil_1859,
  @the_fall_of_the_house_of_usher_1839,
  @the_district_doctor_1852,
  @the_destructors_1954,
  @the_catbird_seat_1942,
  @psyche's_art_1858,
  @master_zacharius_1954,
  @good_country_people_1955,
  @gods_in_exile_1853,
  @first_confession_1950,
  @coming_attraction_1950,
  @bartelby_the_scrivener_1853,
  @a_terribly_strange_bed_1852,
  @a_sound_of_thunder_1952,
  @a_perfect_day_for_bananafish,
  @A_&_P_1951.txt
---

\newpage
# Introduction

This project explores the differences in writing styles between the mid 1800s and the mid 1900s, with a focus on why older texts are often harder to understand. By examining a collection of short stories from both periods, we look at how factors like sentence length, word length, lexical density, and lexical diversity may influence the readability of older texts. This question is interesting because understanding these shifts in writing style can reveal how language and communication have changed over time, and whether these shifts have made texts more accessible or harder to read.

# Data

```{r, include=FALSE}
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
library(sentimentr)
library(knitr)
library(gt)
library(syuzhet)
library(tidytext)
library(dplyr)
source("../R/helper_functions.R")
source("../R/keyness_functions.R")

```

```{r, include = FALSE}
#1850 texts

txt_1842 <- paste(readLines("Text Files/1850/the_masque_of_the_red_death_1842.txt"), collapse = " ")

txt_1859 <- paste(readLines("Text Files/1850/the_lifted_veil_1859.txt"), collapse = " ")

txt_1852 <- paste(readLines("Text Files/1850/the_district_doctor_1852.txt"), collapse = " ")

txt_1852b <- paste(readLines("Text Files/1850/a_terribly_strange_bed_1852.txt"), collapse = " ")

txt_1858 <- paste(readLines("Text Files/1850/psyche's_art_1858.txt"), collapse = " ")

txt_1853b <- paste(readLines("Text Files/1850/bartelby_the_scrivener_1853.txt"), collapse = " ")

txt_1853 <- paste(readLines("Text Files/1850/gods_in_exile_1853.txt"), collapse = " ")

#1875 texts

txt_1877 <- paste(readLines("Text Files/1875/a_simple_soul_1877.txt"), collapse = " ")

txt_1866 <- paste(readLines("Text Files/1875/an_honest_thief_1866.txt"), collapse = " ")

txt_1870 <- paste(readLines("Text Files/1875/ethan_brand_1870.txt"), collapse = " ")

txt_1869 <- paste(readLines("Text Files/1875/mumu_1869.txt"), collapse = " ")

txt_1884 <- paste(readLines("Text Files/1875/rappaccinis_daughter_1884.txt"), collapse = " ")

#txt_1865 <- paste(readLines("Text Files/1875/the_celebrated_jumping_frog_of_calaveras_county_1865).txt"), collapse = " ")

txt_1868 <- paste(readLines("Text Files/1875/the_luck_of_roaring_camp_1868.txt"), collapse = " ")

#1900 texts

txt_1891 <- paste(readLines("Text Files/1900/a_scandal_in_bohemia_1891.txt"), collapse = " ")

txt_1905 <- paste(readLines("Text Files/1900/the_gift_of_the_magi_1905.txt"), collapse = " ")

txt_1902 <- paste(readLines("Text Files/1900/the_monkeys_paw_1902.txt"), collapse = " ")

txt_1897 <- paste(readLines("Text Files/1900/the_open_boat_1897.txt"), collapse = " ")

txt_1894 <- paste(readLines("Text Files/1900/the_story_of_an_hour_1894.txt"), collapse = " ")

txt_1892 <- paste(readLines("Text Files/1900/the_yellow_wallpaper_1892.txt"), collapse = " ")

txt_1908 <- paste(readLines("Text Files/1900/to_build_a_fire_1908.txt"), collapse = " ")

#1925 texts

txt_1922 <- paste(readLines("Text Files/1925/a_hunger_artist_1922.txt"), collapse = " ")

txt_1930 <- paste(readLines("Text Files/1925/a_rose_for_emily_1930.txt"), collapse = " ")

txt_1927 <- paste(readLines("Text Files/1925/hills_like_white_elephants_1927.txt"), collapse = " ")

txt_1919 <- paste(readLines("Text Files/1925/kew_gardens_1919.txt"), collapse = " ")

txt_1922b <- paste(readLines("Text Files/1925/the_diamond_as_big_as_the_ritz_1922.txt"), collapse = " ")

txt_1922c <- paste(readLines("Text Files/1925/the_garden_party_1922.txt"), collapse = " ")

txt_1917 <- paste(readLines("Text Files/1925/the_lottery_ticket_1917.txt"), collapse = " ")

txt_1926 <- paste(readLines("Text Files/1925/the_rocking-horse_winner_1926.txt"), collapse = " ")

#1950 texts

txt_1940 <- paste(readLines("Text Files/1950/the_man_who_killed_the_world_1940.txt"), collapse = " ")

txt_1954b <- paste(readLines("Text Files/1950/master_zacharius_1954.txt"), collapse = " ")

txt_1955 <- paste(readLines("Text Files/1950/good_country_people_1955.txt"), collapse = " ")

txt_1945 <- paste(readLines("Text Files/1950/the_other_side_of_the_hedge_1945.txt"), collapse = " ")

txt_1852 <- paste(readLines("Text Files/1850/the_old_nurses_story_1852.txt"), collapse = " ")

txt_1952 <- paste(readLines("Text Files/1950/the_old_man_at_the_bridge_1952.txt"), collapse = " ")

txt_1950 <- paste(readLines("Text Files/1950/the_mindworm_1950.txt"), collapse = " ")

txt_1950b <- paste(readLines("Text Files/1950/first_confession_1950.txt"), collapse = " ")

txt_1950c <- paste(readLines("Text Files/1950/coming_attraction_1950.txt"), collapse = " ")

txt_1954 <- paste(readLines("Text Files/1950/the_destructors_1954.txt"), collapse = " ")

txt_1942 <- paste(readLines("Text Files/1950/the_catbird_seat_1942.txt"), collapse = " ")

txt_1952b <- paste(readLines("Text Files/1950/a_sound_of_thunder_1952.txt"), collapse = " ")

txt_1948 <- paste(readLines("Text Files/1950/a_perfect_day_for_bananafish_1948.txt"), collapse = " ")

txt_1951 <- paste(readLines("Text Files/1950/A_&_P_1951.txt"), collapse = " ")

```

```{r,include=FALSE}
years_q <- c("1842", "1852", "1852b", "1853", "1853b", "1858", "1859", "1866", "1868", "1869", "1870", "1877", "1884", "1891", "1892", "1894", "1897", "1902", "1905", "1908", "1917", "1919", "1922", "1922b", "1922c", "1926", "1927", "1930", "1940", "1942", "1945", "1948", "1950", "1950b", "1950c", "1951", "1952", "1952b", "1954", "1954b", "1955")

years <- c(txt_1842, txt_1852, txt_1852b, txt_1853, txt_1853b, txt_1858, txt_1859, txt_1866, txt_1868, txt_1869, txt_1870, txt_1877, txt_1884, txt_1891, txt_1892, txt_1894, txt_1897, txt_1902, txt_1905, txt_1908, txt_1917, txt_1919, txt_1922, txt_1922b, txt_1922c, txt_1926, txt_1927, txt_1930, txt_1940, txt_1942, txt_1945, txt_1948, txt_1950, txt_1950b, txt_1950c, txt_1951, txt_1952, txt_1952b, txt_1954, txt_1954b, txt_1955)

years_1850_q <- c("1842", "1852", "1852b", "1853", "1853b", "1858", "1859")

years_1875_q <- c("1866", "1868", "1869", "1870", "1877", "1884")

years_1900_q <- c("1891", "1892", "1894", "1897", "1902", "1905", "1908")

years_1925_q <- c("1917", "1919", "1922", "1922b", "1922c", "1926", "1927", "1930")

years_1950_q <- c("1940", "1942", "1945", "1948", "1950", "1950b", "1950c", "1951", "1952", "1952b", "1954", "1954b", "1955")

years_1850 <- c(txt_1842, txt_1852, txt_1852b, txt_1853, txt_1853b, txt_1858, txt_1859)

years_1875 <- c(txt_1866, txt_1868, txt_1869, txt_1870, txt_1877, txt_1884)

years_1900 <- c(txt_1891, txt_1892, txt_1894, txt_1897, txt_1902, txt_1905, txt_1908)

years_1925 <- c(txt_1917, txt_1919, txt_1922, txt_1922b, txt_1922c, txt_1926, txt_1927, txt_1930)

years_1950 <- c(txt_1940, txt_1942, txt_1945, txt_1948, txt_1950, txt_1950b, txt_1950c, txt_1951, txt_1952, txt_1952b, txt_1954, txt_1954b, txt_1955)


short_stories_corpus <- data.frame(doc_id = years_q, text = years) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()

short_stories_1850_corpus <- data.frame(doc_id = years_1850_q, text = years_1850) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()

short_stories_1875_corpus <- data.frame(doc_id = years_1875_q, text = years_1875) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()

short_stories_1900_corpus <- data.frame(doc_id = years_1900_q, text = years_1900) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()

short_stories_1925_corpus <- data.frame(doc_id = years_1925_q, text = years_1925) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()

short_stories_1950_corpus <- data.frame(doc_id = years_1950_q, text = years_1950) %>%
   mutate(text = preprocess_text(text)) %>%
   corpus()
```

```{r}
### calculation of total words per set of texts

all_text_1850 <- paste(txt_1842, txt_1852, txt_1852b, txt_1853, txt_1853b, txt_1858, txt_1859)
words_1850 <- unlist(strsplit(all_text_1850, "\\s+"))
total_word_count1850s <- length(words_1850)

all_text_1875 <- paste(txt_1866, txt_1868, txt_1869, txt_1870, txt_1877, txt_1884)
words_1875 <- unlist(strsplit(all_text_1875, "\\s+"))
total_word_count1875s <- length(words_1875)

all_text_1900 <- paste(txt_1891, txt_1892, txt_1894, txt_1897, txt_1902, txt_1905, txt_1908)
words_1900 <- unlist(strsplit(all_text_1900, "\\s+"))
total_word_count1900s <- length(words_1900)

all_text_1925 <- paste(txt_1917, txt_1919, txt_1922, txt_1922b, txt_1922c, txt_1926, txt_1927, txt_1930)
words_1925 <- unlist(strsplit(all_text_1925, "\\s+"))
total_word_count1925s <- length(words_1925)

all_text_1950 <- paste(txt_1940, txt_1942, txt_1945, txt_1948, txt_1950, txt_1950b, txt_1950c, txt_1951, txt_1952, txt_1952b, txt_1954, txt_1954b, txt_1955)
words1950s <- unlist(strsplit(all_text_1950, "\\s+"))
total_word_count1950s <- length(words1950s)

word_count_1842 <- length(unlist(strsplit(txt_1842, "\\s+")))
word_count_1852 <- length(unlist(strsplit(txt_1852, "\\s+")))
word_count_1852b <- length(unlist(strsplit(txt_1852b, "\\s+")))
word_count_1853 <- length(unlist(strsplit(txt_1853, "\\s+")))
word_count_1853b <- length(unlist(strsplit(txt_1853b, "\\s+")))
word_count_1858 <- length(unlist(strsplit(txt_1858, "\\s+")))
word_count_1859 <- length(unlist(strsplit(txt_1859, "\\s+")))
word_counts1850s <- c(word_count_1842, word_count_1852, word_count_1852b, word_count_1853, word_count_1853b, word_count_1858, word_count_1859)
average_word_count1850s <- mean(word_counts1850s)

#average word count 1850s: 10120.429
#average word count 1875s: 8694.500
#average word count 1900s: 5426.000
#average word count 1925s: 4905.375
#average word count 1950s: 5379.615
```

```{r}
word_count_1866 <- length(unlist(strsplit(txt_1866, "\\s+")))
word_count_1868 <- length(unlist(strsplit(txt_1868, "\\s+")))
word_count_1869 <- length(unlist(strsplit(txt_1869, "\\s+")))
word_count_1870 <- length(unlist(strsplit(txt_1870, "\\s+")))
word_count_1877 <- length(unlist(strsplit(txt_1877, "\\s+")))
word_count_1884 <- length(unlist(strsplit(txt_1884, "\\s+")))

word_counts1875s <- c(word_count_1866, word_count_1868, word_count_1869, word_count_1870, word_count_1877, word_count_1884)
average_word_count1875s <- mean(word_counts1875s)
```

```{r}
word_count_1891 <- length(unlist(strsplit(txt_1891, "\\s+")))
word_count_1892 <- length(unlist(strsplit(txt_1892, "\\s+")))
word_count_1894 <- length(unlist(strsplit(txt_1894, "\\s+")))
word_count_1897 <- length(unlist(strsplit(txt_1897, "\\s+")))
word_count_1902 <- length(unlist(strsplit(txt_1902, "\\s+")))
word_count_1905 <- length(unlist(strsplit(txt_1905, "\\s+")))
word_count_1908 <- length(unlist(strsplit(txt_1908, "\\s+")))

word_counts1900s <- c(word_count_1891, word_count_1892, word_count_1894, word_count_1897, word_count_1902, word_count_1905, word_count_1908)
average_word_count1900s <- mean(word_counts1900s)
```

```{r}
word_count_1917 <- length(unlist(strsplit(txt_1917, "\\s+")))
word_count_1919 <- length(unlist(strsplit(txt_1919, "\\s+")))
word_count_1922 <- length(unlist(strsplit(txt_1922, "\\s+")))
word_count_1922b <- length(unlist(strsplit(txt_1922b, "\\s+")))
word_count_1922c <- length(unlist(strsplit(txt_1922c, "\\s+")))
word_count_1926 <- length(unlist(strsplit(txt_1926, "\\s+")))
word_count_1927 <- length(unlist(strsplit(txt_1927, "\\s+")))
word_count_1930 <- length(unlist(strsplit(txt_1930, "\\s+")))

word_counts1925s <- c(word_count_1917, word_count_1919, word_count_1922, word_count_1922b, word_count_1922c, word_count_1926, word_count_1927, word_count_1930)
average_word_count1925s <- mean(word_counts1925s)
```

```{r}
word_count_1940 <- length(unlist(strsplit(txt_1940, "\\s+")))
word_count_1942 <- length(unlist(strsplit(txt_1942, "\\s+")))
word_count_1945 <- length(unlist(strsplit(txt_1945, "\\s+")))
word_count_1948 <- length(unlist(strsplit(txt_1948, "\\s+")))
word_count_1950 <- length(unlist(strsplit(txt_1950, "\\s+")))
word_count_1950b <- length(unlist(strsplit(txt_1950b, "\\s+")))
word_count_1950c <- length(unlist(strsplit(txt_1950c, "\\s+")))
word_count_1951 <- length(unlist(strsplit(txt_1951, "\\s+")))
word_count_1952 <- length(unlist(strsplit(txt_1952, "\\s+")))
word_count_1952b <- length(unlist(strsplit(txt_1952b, "\\s+")))
word_count_1954 <- length(unlist(strsplit(txt_1954, "\\s+")))
word_count_1954b <- length(unlist(strsplit(txt_1954b, "\\s+")))
word_count_1955 <- length(unlist(strsplit(txt_1955, "\\s+")))

word_counts1950s <- c(word_count_1940, word_count_1942, word_count_1945, word_count_1948, word_count_1950, word_count_1950b, word_count_1950c, 
                      word_count_1951, word_count_1952, word_count_1952b, 
                      word_count_1954, word_count_1954b, word_count_1955)
average_word_count1950s <- mean(word_counts1950s)
```

```{r}
word_count_range1850s <- range(word_counts1850s)
word_count_range1875s <- range(word_counts1875s)
word_count_range1900s <- range(word_counts1900s)
word_count_range1925s <- range(word_counts1925s)
word_count_range1950s <- range(word_counts1950s)
#1850s range 2396 17574
#1875s range 4108 12081
#1900s range 1005 9298
#1925s range 1460 14819
#1950s range 698 17321
```

```{r}
data <- data.frame(
  Year = c(1839, 1842, 1852, 1852, 1853, 1853, 1858, 1859, 1940, 1942,
           1945, 1948, 1950, 1950, 1950, 1951, 1952, 1952, 1954, 1954, 1955),
  ASL = c(32.958716, 24.089109, 9.452273, 28.935211, 28.892704, 20.823613, 26.641892, 
          31.801418, 12.713514, 12.625000, 21.357798, 14.830313, 9.020110, 19.818182, 
          11.798595, 24.065574, 12.590164, 9.076132, 10.485348, 22.316456, 14.430213),
  AWL = c(4.581013, 4.498561, 3.907167, 4.320058, 4.607382, 4.348931, 4.229521, 
          4.417706, 4.331188, 4.116343, 3.994943, 3.942544, 4.325866, 3.859438, 
          4.184095, 3.960326, 3.658392, 4.101703, 4.006108, 4.400313, 3.929711)
)

data_1850s <- subset(data, Year >= 1839 & Year <= 1859)
data_1950s <- subset(data, Year >= 1940 & Year <= 1955)

avg_1850s <- colMeans(data_1850s[, c("ASL", "AWL")])
avg_1950s <- colMeans(data_1950s[, c("ASL", "AWL")])
```

The corpora used for this project are 5 collections of short stories from the mid 1800s to the mid 1900s. Each collection spans around 25 years. For example, the first collection spans from around the 1850s to 1875. Given the dates chosen, almost all available texts are in the public domain and relatively easily accessible. The stories chosen come from a variety of genres and authors and are generally highly rated. Despite the small sample size, the selection of texts attempts to capture literary trends of their times. The stories were primarily sourced from Project Gutenberg, an online repository of over 70,000 eBooks. This site was chosen because it was simple to copy and paste the stories into individual text files for analysis.

The corpus was tokenized by words, where each token represents an individual word. Tokens were standardized by lowercasing and removing special characters, punctuation, etc. There are a total of 44 documents or short stories in the corpus. 

```{r}
# Function to calculate average word length
calculate_avg_word_length <- function(text) {
  words <- unlist(strsplit(text, "\\s+"))
  total_chars <- sum(nchar(gsub("[[:punct:][:space:]]", "", words)))
  avg_word_length <- total_chars / length(words)
  return(avg_word_length)
}

# Function to calculate average sentence length
calculate_avg_sentence_length <- function(text) {
  sentences <- unlist(strsplit(text, "[.!?]+"))
  words <- unlist(strsplit(text, "\\s+"))
  avg_sentence_length <- length(words) / length(sentences)
  return(avg_sentence_length)
}

# Calculate metrics for each range
metrics_by_range <- function(texts) {
  avg_word_lengths <- sapply(texts, calculate_avg_word_length)
  avg_sentence_lengths <- sapply(texts, calculate_avg_sentence_length)
  return(list(avg_word_length = mean(avg_word_lengths),
              avg_sentence_length = mean(avg_sentence_lengths)))
}

# Example for 1850s
metrics_1850s <- metrics_by_range(c(txt_1842, txt_1852, txt_1852b, txt_1853, txt_1853b, txt_1858, txt_1859))
metrics_1875s <- metrics_by_range(c(txt_1866, txt_1868, txt_1869, txt_1870, txt_1877, txt_1884))
metrics_1900s <- metrics_by_range(c(txt_1891, txt_1892, txt_1894, txt_1897, txt_1902, txt_1905, txt_1908))
metrics_1925s <- metrics_by_range(c(txt_1917, txt_1919, txt_1922, txt_1922b, txt_1922c, txt_1926, txt_1927, txt_1930))
metrics_1950s <- metrics_by_range(c(txt_1940, txt_1942, txt_1945, txt_1948, txt_1950, txt_1950b, txt_1950c, txt_1951, txt_1952, txt_1952b, txt_1954, txt_1954b, txt_1955))

```


```{r}
data <- data.frame(
  Decade =c("Mid 1800s", "Late 1800s", "1900s", "Early 1900s", "Mid 1900s"),
  Text_Count = c(9, 7, 7, 8, 13),
    Total_Word_Count = c(total_word_count1850s, total_word_count1875s, total_word_count1900s, total_word_count1925s, total_word_count1950s),
  Average_Word_Count = c(average_word_count1850s, average_word_count1875s, average_word_count1900s, average_word_count1925s, average_word_count1950s),
  Word_Range = c(
    paste(word_count_range1850s[1], word_count_range1850s[2], sep = " - "),
    paste(word_count_range1875s[1], word_count_range1875s[2], sep = " - "),
    paste(word_count_range1900s[1], word_count_range1900s[2], sep = " - "),
    paste(word_count_range1925s[1], word_count_range1925s[2], sep = " - "),
    paste(word_count_range1950s[1], word_count_range1950s[2], sep = " - ")
  ),
    Avg_Sentence_Length = c(metrics_1850s$avg_sentence_length, metrics_1875s$avg_sentence_length,
                          metrics_1900s$avg_sentence_length, metrics_1925s$avg_sentence_length,
                          metrics_1950s$avg_sentence_length),
    Avg_Word_Length = c(metrics_1850s$avg_word_length, metrics_1875s$avg_word_length,
                      metrics_1900s$avg_word_length, metrics_1925s$avg_word_length,
                      metrics_1950s$avg_word_length)
  )

kable(data, caption = "Word Count Data for Mid 1800s and Mid 1900s Texts", align = 'c', 
      col.names = c("Time Period", "Texts", "Total Words", "Mean Words", "Word Range", "Average Sentence Length", "Average Word Length"))
```
From Table 1, we observe that the number of texts per time period is relatively even and well-distributed. However, the total word counts and mean word counts for each time period show greater variation. Specifically, the average number of words per short story tends to decrease over time. Additionally, stories from the 1800s generally exhibit longer average sentence lengths compared to those from the 1900s. In contrast, the average word length appears consistent across the time periods. Overall these corpora have similar amounts of content that can be used to draw conclusions.

# Methods

For our initial analysis, we wanted to determine the complexity of the texts from the different time periods. In order to do so, we analyzed two different common measurements for lexical complexity - lexical density and lexical diversity (@ZHOU2023101262).

First, average sentence and word lengths of each of the texts from the mid 1800s and mid 1900s were examined. This was done by analyzing histograms for each period, which offered a general overview of the two comparable periods.

Average lexical density and average lexical diversity were then examined for the two periods. Lexical density and diversity were chosen because they are measurements for lexical complexity. The averages were used in order to draw a comparison between time periods.

Lexical density is a measure of the proportion of content words to function words in a text. Content words, which include nouns, verbs, adjectives, etc, are words that carry meaning in a sentence. On the other hand, function words, such as articles, conjunctions, prepositions, etc, are used for grammatical purposes. Lexical density is higher when the proportion of content words is greater than that of function words.

We examined lexical diversity using three different measurements - Carroll's Corrected Type Token Ratio (CTTR) (@cttr), Uber's Index (@uberIndex, @uberIndex2), and Yule's K (@yuleK). We used these measurements to provide confirmation on the diversity analysis, and they were chosen because they all account and correct for text length sensitivities. Yule's K was chosen because it is also sensitive to frequency distributions. A low score for CTTR and Uber's Index typically indicates lower diversity when compared to a higher score. A high Yule's K score typically indicates low lexical diversity. The high score also signifies that unique words are repeated more often within the texts. For certain measurements, we further expanded our analysis by viewing the CTTR and the Uber's Index scores through a histogram. This gave us an understanding of the trends among the individual texts within each period.

# Results

## Average Sentence and Word Length

```{r, fig.cap="Distributions of Average Sentence Length", out.height = "30%"}
par(mfrow = c(1, 2)) 
#### Calculations of mean sentence and word length

calculate_asl <- function(text) {
   unlist(str_split(text, pattern = "\\.\\s*")) %>%
    sapply(function(x) str_count(x, "\\w+")) %>%
    mean()
}


calculate_awl <- function(tokens) {
  tokens %>%
    tokens(what = "word") %>%
    as.list() %>%
    unlist() %>%
    nchar() %>%
    mean()
}


# Histogram for the 1850s
hist(data_1850s$ASL, 
     main = "Avg. Sentence Length: Mid 1800s", 
     xlab = "Average Sentence Length", 
     col = "lightblue", 
     border = "black", 
     breaks = 5, 
     cex.main = 0.90)  

# Histogram for the 1950s
hist(data_1950s$ASL, 
     main = "Avg. Sentence Length: Mid 1900s", 
     xlab = "Average Sentence Length", 
     col = "lightgreen", 
     border = "black", 
     breaks = 5, 
     cex.main = 0.90)  # Adjust title font size

```

```{r, fig.cap="Distributions of Average Word Length", out.height = "30%"}
par(mfrow = c(1, 2)) 

# Histogram for the 1850s
hist(data_1850s$AWL, 
     main = "Avg. Word Length: Mid 1800s", 
     xlab = "Average Word Length", 
     col = "lightblue", 
     border = "black", 
     breaks = 5, 
     cex.main = 0.90)

# Histogram for the 1950s
hist(data_1950s$AWL, 
     main = "Avg. Word Length: Mid 1900s", 
     xlab = "Average Word Length", 
     col = "lightgreen", 
     border = "black", 
     breaks = 5, 
     cex.main = 0.90)

```

The histograms in Figure 1 reveal that most texts from the mid 1800s have an average sentence length between 20 and 35 words, indicating a potential preference for longer sentences during this period. In contrast, the distribution of average sentence lengths in the mid 1900s is more even, with most of the texts falling between 0 and 15 words, reflecting a possible shift towards shorter sentences.

As shown in the histograms in Figure 2, the distributions for the mid 1800s and mid 1900s are similar. In both time periods, the average word length ranges from 3.6 to 4.8 characters per word.

## Lexical Density

```{r}
# Calculate lexical density for each text

calculate_lexical_density <- function(texts) {
  token <- tokens(texts)
  token_lex <- tokens_select(token, pattern=stopwords("en"), selection = "remove")
  total_words <- ntoken(token)
  lexical_words <- ntoken(token_lex)
  lex_density <- lexical_words / total_words
  
  return(lex_density)
}

tokens_list <- tokens(short_stories_corpus)
tokens_lexical <- tokens_select(tokens_list, pattern = stopwords("en"), selection = "remove")

total_words <- ntoken(tokens_list)
lexical_words <- ntoken(tokens_lexical)
lexical_density <- lexical_words / total_words

density_1850 <- calculate_lexical_density(all_text_1850)
density_1875 <- calculate_lexical_density(all_text_1875)
density_1900 <- calculate_lexical_density(all_text_1900)
density_1925 <- calculate_lexical_density(all_text_1925)
density_1950 <- calculate_lexical_density(all_text_1950)
Lexical_Density <- c(density_1850[[1]], density_1875[[1]], density_1900[[1]], density_1925[[1]], density_1950[[1]])

lexical_density_data <- data.frame(
  Decade = c("Mid 1800s", "Late 1800s", "1900s", "Early 1900s", "Mid 1900s"),
  Lexical_Density = Lexical_Density
)

colnames(lexical_density_data) <- c("Time Period", "Lexical Diversity")

kable(lexical_density_data, caption = "Average Lexical Density for the Mid 1800s and Mid 1900s")

```

From Table 2, the average lexical density of the texts from the mid-1800s is 0.5539, compared to 0.5768 in the mid-1900s. The intermediate values between these periods also show a general upward trend. This indicates a slight increase in lexical density over time, suggesting a slight rise in the proportion of content words in the mid 1900s texts. However, given the minimal differences between the time periods, this change is unlikely to have a significant impact on the readability or complexity of the texts. A larger dataset would be needed to draw more definitive conclusions, since our sample size is relatively small.

## Lexical Diversity

```{r}
dfm_short_stories <- dfm_remove(dfm(tokens(short_stories_corpus)), stopwords("en"))

dfm_short_stories_1850 <- dfm_remove(dfm(tokens(short_stories_1850_corpus)), stopwords("en"))

dfm_short_stories_1875 <- dfm_remove(dfm(tokens(short_stories_1875_corpus)), stopwords("en"))

dfm_short_stories_1900 <- dfm_remove(dfm(tokens(short_stories_1900_corpus)), stopwords("en"))

dfm_short_stories_1925 <- dfm_remove(dfm(tokens(short_stories_1925_corpus)), stopwords("en"))

dfm_short_stories_1950 <- dfm_remove(dfm(tokens(short_stories_1950_corpus)), stopwords("en"))

lexdiv_ss_1850 <- textstat_lexdiv(dfm_short_stories_1850, measure = c("CTTR", "U", "K"))

lexdiv_ss_1875 <- textstat_lexdiv(dfm_short_stories_1875, measure = c("CTTR", "U", "K"))

lexdiv_ss_1900 <- textstat_lexdiv(dfm_short_stories_1900, measure = c("CTTR", "U", "K"))

lexdiv_ss_1925 <- textstat_lexdiv(dfm_short_stories_1925, measure = c("CTTR", "U", "K"))

lexdiv_ss_1950 <- textstat_lexdiv(dfm_short_stories_1950, measure = c("CTTR", "U", "K"))


### CTTR is the Corrected Type Token Ratio --> tells us lexical diversity corrected for text length

### U is the Uber Index --> this is another measure of lexical richness and diversity which is less sensitive to text length

### K is Yule's K --> quantifies the unevenness in the frequency distribution of words in a text; focuses on the distribution of unique words' frequencies

cttr <- function(lexdiv) {
  return(round(mean(lexdiv$CTTR), 3))
}

u <- function(lexdiv) {
  return(round(mean(lexdiv$U), 3))
}

k <- function(lexdiv) {
  return(round(mean(lexdiv$K), 3))
}

Decade = c("Mid 1800s", "Late 1800s", "1900s", "Early 1900s", "Mid 1900s")
CTTR = c(cttr(lexdiv_ss_1850), cttr(lexdiv_ss_1875), cttr(lexdiv_ss_1900), cttr(lexdiv_ss_1925), cttr(lexdiv_ss_1950))
Uber_Index = c(u(lexdiv_ss_1850), u(lexdiv_ss_1875), u(lexdiv_ss_1900), u(lexdiv_ss_1925), u(lexdiv_ss_1950))
Yule_Index = c(k(lexdiv_ss_1850), k(lexdiv_ss_1875), k(lexdiv_ss_1900), k(lexdiv_ss_1925), k(lexdiv_ss_1950))

lex_div_res <- data.frame(Decade,CTTR,Uber_Index,Yule_Index)

colnames(lex_div_res) <- c("Time Period", "CTTR", "Uber's Index", "Yule's K")


kable(lex_div_res, caption = "Average Lexical Diversity for the Mid 1800s and Mid 1900s")

```

As seen in Table 3, for the CTTR and Uber's Index, the value from the mid 1800s is higher than that of the mid 1900s. This indicates a decrease in lexical diversity between the two time periods. The Yule's K value for the mid 1800s is 14.270 where the Yule's K value for the mid 1900s is 31.118. This suggests that the texts from the mid 1900s have a more uniform distribution of word frequencies and thus less lexical diversity. 

```{r}
# t test for CTTR for consecutive periods
t_test_1850_75 <- t.test(lexdiv_ss_1850$CTTR, lexdiv_ss_1875$CTTR)
t_test_18_19 <- t.test(lexdiv_ss_1875$CTTR, lexdiv_ss_1900$CTTR)
t_test_1900_25 <- t.test(lexdiv_ss_1900$CTTR, lexdiv_ss_1925$CTTR)
t_test_1925_50 <- t.test(lexdiv_ss_1925$CTTR, lexdiv_ss_1950$CTTR)

# t test for CTTR for end periods
t_test_1850_1950 <- t.test(lexdiv_ss_1850$CTTR, lexdiv_ss_1950$CTTR)

#Uber's Index t test for consecutive periods
u_t_test_1850_75 <- t.test(lexdiv_ss_1850$U, lexdiv_ss_1875$U)
u_t_test_18_19 <- t.test(lexdiv_ss_1875$U, lexdiv_ss_1900$U)
u_t_test_1900_25 <- t.test(lexdiv_ss_1900$U, lexdiv_ss_1925$U)
u_t_test_1925_50 <- t.test(lexdiv_ss_1925$U, lexdiv_ss_1950$U)

#Uber's index end periods
u_t_test_1850_1950 <- t.test(lexdiv_ss_1850$U, lexdiv_ss_1950$U)

#Yule's K t test for consecutive periods
K_t_test_1850_75 <- t.test(lexdiv_ss_1850$K, lexdiv_ss_1875$K)
K_t_test_18_19 <- t.test(lexdiv_ss_1875$K, lexdiv_ss_1900$K)
K_t_test_1900_25 <- t.test(lexdiv_ss_1900$K, lexdiv_ss_1925$K)
K_t_test_1925_50 <- t.test(lexdiv_ss_1925$K, lexdiv_ss_1950$K)

#Yule's K end periods
K_t_test_1850_1950 <- t.test(lexdiv_ss_1850$K, lexdiv_ss_1950$K)

```

```{r, fig.cap="Distributions of CTTR and Uber's Index", out.height = "40%"}
par(mfrow = c(2, 2))  # Layout for 2x2 histograms

# Histogram for CTTR in 1800s
hist(lexdiv_ss_1850$CTTR, 
     main = "CTTR: Mid 1800s", 
     xlab = "CTTR", 
     col = "lightblue", 
     border = "black", 
     breaks = 10)  

# Histogram for CTTR in 1900s
hist(lexdiv_ss_1950$CTTR, 
     main = "CTTR: Mid 1900s", 
     xlab = "CTTR", 
     col = "lightblue", 
     border = "black", 
     breaks = 10)  

# Histogram for Uber's Index in 1800s
hist(lexdiv_ss_1850$U, 
     main = "Uber's Index: Mid 1800s", 
     xlab = "Uber's Index", 
     col = "lightgreen", 
     border = "black", 
     breaks = 10)  

# Histogram for Uber's Index in 1950s
hist(lexdiv_ss_1950$U, 
     main = "Uber's Index: Mid 1900s", 
     xlab = "Uber's Index", 
     col = "lightgreen", 
     border = "black", 
     breaks = 10)  
```

For the histograms in Figure 3, we can see that there is no noticeable curves, indicating mostly uniform distributions. Notably, the spread for both measures for both years is very similar. Thus, while there are specific texts within each period that greatly contribute to the previously discussed average values, many of the texts are comparably similar in lexical diversity.

# Discussion

From our exploration, we determined that the lexical density showed a small increase over time where the lexical diversity showed a decrease over time. However, there was a minimal difference between the two period's lexical density, which might a less significant impact. The histograms also showed that sentences from the mid 1800s were on average longer than sentences from the mid 1900s. Thus, there is a likely decrease in lexical complexity of short stories between the two time periods, potentially explaining a wider preference for short stories from later time periods. 

However, it is important to note that due to the small sample size, we do not have enough significance to make any conclusive statements. Thus, some limitations to consider are the relatively small size of our dataset, consisting of only about 25 texts. Expanding the dataset and incorporating a broader range of time periods beyond the mid 1800s and mid 1900s would allow us to make more meaningful statements about trends. Additionally, the texts were not randomly sampled, as we selected the top short stories suggested by ChatGPT. Ideally, a larger and more randomly selected sample would allow for more reliable conclusions about trends. 

A future analysis could include more time periods and a larger variety of texts to improve the generalizability of our findings. Focusing on specific genres, such as romance or science-fiction, would allow for a better understanding of how different types of writing have evolved over time. This would help us understand whether certain genres have been more resistant to changes in style or if they have had the same characteristics over time. Additionally, analyzing how historical events or societal shifts influence language and writing styles could provide insight into how external factors influence the way people communicate.


# Acknowledgments {.appendix}

ChatGPT was used to supplement our lack of knowledge of lexical analysis and finding short stories. We asked the LLM to describe some of the measures for lexical diversity and density we were considering using. We also asked it for recommendations of short stories from our two chosen time periods. The LLM was helpful in both regards as it gave us useful information on our statistical measures and pointed us towards resources we could use for building our corpus. We also utilized the texstat page for developing ideas and creating tables (@WinNT).

# 7  Works Cited
