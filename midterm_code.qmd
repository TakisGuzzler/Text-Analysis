
---
title: "Your Title Goes Here"
subtitle: "A report to the Quantum Insight AI research team."
author: "My Name"
date: last-modified
format:
  pdf:
    number-sections: true
    indent: true
    toc: false
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
execute:
  echo: false
---

# Statement of the Problem



# Summary of Findings



# Recommendations



# Appendix

```{r, include=FALSE}
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
library(knitr)
library(gt)
library(syuzhet)
library(dplyr)
library(nFactors)
source("../R/helper_functions.R")
source("../R/keyness_functions.R")
source("../R/mda_functions.R")

```

```{r}
### Generate arxiv sub DFM from SPACY ####

arxiv_spacy <- read.table("arxiv_sample/arxiv_spacy.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)

anno_edit_arxiv <- structure(arxiv_spacy, class = c("spacyr_parsed", "data.frame"))

subtkns_arxiv <- as.tokens(anno_edit_arxiv, include_pos = "tag", concatenator = "")
doc_categories_arxiv <- names(subtkns_arxiv) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))

docvars(subtkns_arxiv) <- doc_categories_arxiv

sub_dfm_arxiv <- subtkns_arxiv %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_arxiv, n = 10) |>
  gt() |>
  as_raw_html()

```

```{r}
### Generate ML3s sub DFM from SPACY ####

meta_llama_3_spacy <- read.table("hape_sample/spacy_data/spacy_Meta-Llama-3-8B.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)

anno_edit_ml3s <- structure(meta_llama_3_spacy, class = c("spacyr_parsed", "data.frame"))

subtkns_ml3s <- as.tokens(anno_edit_ml3s, include_pos = "tag", concatenator = "")
doc_categories_ml3s <- names(subtkns_ml3s) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))

docvars(subtkns_ml3s) <- doc_categories_ml3s

sub_dfm_ml3s <- subtkns_ml3s %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_ml3s, n = 10) |>
  gt() |>
  as_raw_html()

```

```{r}
### Generate chatGPT4.0 sub DFM from SPACY ####

gpt_4o_spacy <- read.table("hape_sample/spacy_data/spacy_gpt-4o-2024-08-06.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)

anno_edit_gpt4o <- structure(gpt_4o_spacy, class = c("spacyr_parsed", "data.frame"))

subtkns_gpt4o <- as.tokens(anno_edit_gpt4o, include_pos = "tag", concatenator = "")

doc_categories_gpt4o <- names(subtkns_gpt4o) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))

docvars(subtkns_gpt4o) <- doc_categories_gpt4o

sub_dfm_gpt4o <- subtkns_gpt4o %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_gpt4o, n = 10) |>
  gt() |>
  as_raw_html()

```

```{r}
### Generate chunk sub DFM from SPACY ####

chunk_spacy <- read.table("hape_sample/spacy_data/spacy_chunk_2.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)

anno_edit_chunk <- structure(chunk_spacy, class = c("spacyr_parsed", "data.frame"))

subtkns_chunk <- as.tokens(anno_edit_chunk, include_pos = "tag", concatenator = "")
doc_categories_chunk <- names(subtkns_chunk) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))

docvars(subtkns_chunk) <- doc_categories_chunk

sub_dfm_chunk <- subtkns_chunk %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_chunk, n = 10) |>
  gt() |>
  as_raw_html()

```


```{r, include = FALSE}
####read in biber data ####
human_chunk_biber <- read_tsv("hape_sample/biber_data/biber_chunk_2.tsv")

gpt_4o_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-2024-08-06.tsv")

gpt_4o_mini_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-mini-2024-07-18.tsv")

meta_llama_3_8b_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B.tsv")

meta_llama_3_8b_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B-Instruct.tsv")

meta_llama_3_70B_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B.tsv")

meta_llama_3_70B_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B-Instruct.tsv")

arxiv_biber <- read_tsv("arxiv_sample/arxiv_biber.tsv")
```

```{r}

arxiv_biber_human <- arxiv_biber %>% 
  filter(str_detect(doc_id, "human")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

arxiv_biber_machine <- arxiv_biber %>% 
  filter(str_detect(doc_id, "machine")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

# only filter arxiv out 

```

```{r}
bc_cor <- cor(arxiv_biber_human[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(arxiv_biber_machine[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```

```{r}
bc_cor <- cor(human_chunk_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(gpt_4o_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(gpt_4o_mini_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(meta_llama_3_8b_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(meta_llama_3_8b_Instruct_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(meta_llama_3_70B_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```
```{r}
bc_cor <- cor(meta_llama_3_70B_Instruct_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.5)
```


```{r}
screeplot_mda(human_chunk_biber)
```


```{r, include=FALSE}
#read in text cunks for analysis

text_chunk <- read_tsv("hape_sample/text_data/text_chunk_2.tsv")

```


General comments for interpreting the correlational plots -- delete later --

Positive Correlation means that as one linguistic feature increases, so too does the other

Negative correlation means that as one linguistic feature increases, the other decreases

Zero correlation means that there is no linear relationship between the features

As a general example for interpreting the graphs, if the feature nouns (might or might not be an actual feature, this is an example not an actual comment) and adjectives show a dark blue color, then the texts where nouns are frequent also tend to use more adjectives. So, when we extrapolate this to new texts or new tests, maybe we are expecting texts with a lot of nouns to have a lot of adjectives and thus might miss several test cases where this is not the case.

On the flip side, a dark red color would show a negative correlation which could mean???


what does a high negative correlation establish? high positive? why do some graphs have very minimal correlation overall. Note that this is for the entire data set --> if you have a lot of a similar writing style, how might this reflect in the graphs and what might this say about our models. As an example, if the models are highly trained to recognize a specific pattern of relationships, would they do well at then recognizing a different pattern? Are there patterns accross the biber features?

I don't know how relevant this stuff is, but it is food for thought, so to speak.

For these interpretations, context is important. If you are going to comment on the relationship of a text set, might as well also look at the data itself to see if there are any visual patterns? Is there anything to suggest that the model is more used to formal/informal language, etc...

