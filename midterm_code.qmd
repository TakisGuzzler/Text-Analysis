
---
title: "An Investigation into Model Performance for Identification of Human vs. AI text"
subtitle: "A report to the Quantum Insight AI research team."
author: "Jenny, Lauren, Amalia"
date: last-modified
format:
  pdf:
    number-sections: true
    indent: true
    toc: false
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
execute:
  echo: false
---

# Statement of the Problem

Quantum Insight is aiming to distinguish human-authored text from AI-generated content using a parallel corpus of human and machine-generated text. The initial models trained with Biber’s linguistic features performed well in identifying AI-generated text on the training set, achieving high in-sample accuracy. However, when applied to new data from a different corpus, model accuracy decreased significantly. Our goal is to determine the factors contributing to this performance drop and recommend ways to improve model accuracy across different corpora. 

# Summary of Findings

We initially determined a subset of the original models we would investigate. These models were chosen because they were representative of their LLM type and had the smallest and largest differences in overall accuracy and loss of accuracy. We first used the varImp function to identify the most important features for classification in a subset of the original models. We then used correlation matrices to find commonalities between the relationships of features belonging to human-written and AI-generated texts.

A problem arises when a comparison is drawn for the correlation between the top Biber features used for classification by each of the analyzed models in Tables 1, 2, and 3 and other features. Many of the classification features were similarly positively and negatively correlated with similar features across all relevant data sets. This issue can be highlighted by observing the ChatGPT 4o model, where nominalization, present participle, and mean word length become important. As expressed in Figures 1 and 2, nominalization is similarly positively correlated with phrasal coordination, and mean word length is similarly positively and negatively correlated with adj_attr and other_nouns, respectively. This lack of significant difference could have led to confusion during classification. 

Furthermore, for the important features in which differences can be observed (e.g. present participle), similar relationships cannot be distinguished within the original data. We discovered similar problems with the Meta Llama 70B and Meta Llama 70B Instruct models with an additional notable problem being the past tense feature. This feature was categorized as important for the model according to Table 2, but was not present in the out-of-sample data. This discrepancy is due to the prompts for the out of sample data requesting that all responses be worded in “active voice”, which resulted in a loss of categorizing capabilities for the Meta Llama 70B Instruct model.

Following this line of thought, we examined the text to determine whether the prompts could have contributed to the similar correlations across samples. A potential contributing factor was that the prompts for the initial data sets were to "extend" a human-generated text. This specification is highly likely to have resulted in limited differences between the machine and human data sets, which can be observed in Figures 3, 4, 5, and 6 and Tables 4, 5, and 6. There are a limited amount of discrepancies between the LLMs and the humans, though the greatest differences can be observed in the ChatGPT 4o models (Figure 4). Overall, the models trained to look for very specific patterns rather than something broad and widely applicable.


# Recommendations

Future research could involve collecting new training and testing datasets similar to the out of sample dataset where human and machine text were generated through open-ended prompts. This could result in more variance between the human and machine texts. We could also investigate different models for classification. Currently the models are random forests which could be more susceptible to over-fitting with large and over-specified data. Training the models using lasso regression or other machine learning models might improve performance. Additionally, this data was generated by a previous version of ChatGPT, so it could be that ChatGPT performs well because it has a very specific style of writing. Comparing performance on machine generated text from other LLMs could generate more insights into the problems seen in our models.



\newpage
# Appendix

```{r, include=FALSE}
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
library(knitr)
library(gt)
library(syuzhet)
library(dplyr)
library(nFactors)
library(randomForest)
library(caret)
library(kableExtra)
library(corrplot)
library(gridExtra)
library(ggplotify)
library(cowplot)
source("../R/helper_functions.R")
source("../R/keyness_functions.R")
source("../R/mda_functions.R")
load("models/rf_models.rda")
```


```{r}
biber_test <- read_tsv("arxiv_sample/arxiv_biber.tsv", 
                       show_col_types = FALSE)

model_names <- c("ChatGPT", "Meta LLaMA 70B Instruct", "Meta LLaMA 70B")
selected_model_indices <- c(1, 3, 4)  
var_importance_results <- list()

var_importance_results <- list()
for (i in seq_along(selected_model_indices)) {
  rf <- rf_models[[selected_model_indices[i]]]
  importance <- varImp(rf, scale = FALSE)
  model_name <- model_names[i]
  
  var_importance_results[[model_name]] <- as.data.frame(importance$importance) %>%
    rownames_to_column("Feature") %>%
    arrange(desc(Overall)) %>%
    slice(1:5) %>%
    select(Feature, Overall)
}
```

```{r}
kable(var_importance_results["ChatGPT"], caption = "Top 5 Important Features for ChatGPT") %>% kable_styling(latex_options = "striped") %>%
 kable_styling(latex_options = "HOLD_position")

kable(var_importance_results["Meta LLaMA 70B Instruct"], caption = "Top 5 Important Features for Meta Llama 70B  Instruct") %>% kable_styling( latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")


kable(var_importance_results["Meta LLaMA 70B"], caption = "Top 5 Important Features for Meta Llama 70B") %>% kable_styling(latex_options = "striped")  %>% kable_styling(latex_options = "HOLD_position")

```

```{r, include = FALSE}
####read in biber data ####
human_chunk_biber <- read_tsv("hape_sample/biber_data/biber_chunk_2.tsv")

gpt_4o_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-2024-08-06.tsv")

gpt_4o_mini_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-mini-2024-07-18.tsv")

meta_llama_3_8b_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B.tsv")

meta_llama_3_8b_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B-Instruct.tsv")

meta_llama_3_70B_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B.tsv")

meta_llama_3_70B_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B-Instruct.tsv")

arxiv_biber <- read_tsv("arxiv_sample/arxiv_biber.tsv")
```

```{r}

arxiv_biber_human <- arxiv_biber %>% 
  filter(str_detect(doc_id, "human")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

arxiv_biber_machine <- arxiv_biber %>% 
  filter(str_detect(doc_id, "machine")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

# only filter arxiv out 

```

```{r, fig.cap="Correlation Matrices of Out-Sample Human and Machine", fig.align = "center", echo = FALSE, message = FALSE, warning = FALSE}

bc_cor_human <- cor(arxiv_biber_human[-1], method = "pearson")
p1 <- as.ggplot(~corrplot(bc_cor_human, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

bc_cor_machine <- cor(arxiv_biber_machine[-1], method = "pearson")
p2 <- as.ggplot(~corrplot(bc_cor_machine, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

plot_grid(p1, p2, ncol = 2, labels = c("Human", "Machine"))
```

```{r, fig.cap="Correlation Matrices of In-Sample Human and GPT-4o"}
bc_cor_human <- cor(human_chunk_biber[-1], method = "pearson")
p1 <- as.ggplot(~corrplot(bc_cor_human, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

# Generate and convert the GPT-4 correlation plot to ggplot-compatible format
bc_cor_gpt4 <- cor(gpt_4o_biber[-1], method = "pearson")
p2 <- as.ggplot(~corrplot(bc_cor_gpt4, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

# Arrange side by side
plot_grid(p1, p2, ncol = 2, labels = c("Human", "GPT-4o"))
```


```{r, fig.cap="Correlation Matrices of Meta 70B and Meta 70B Instruct Models", echo = FALSE, message = FALSE, warning = FALSE}
bc_cor_meta <- cor(meta_llama_3_70B_biber[-1], method = "pearson")
p1 <- as.ggplot(~corrplot(bc_cor_meta, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

bc_cor_meta_instruct <- cor(meta_llama_3_70B_Instruct_biber[-1], method = "pearson")
p2 <- as.ggplot(~corrplot(bc_cor_meta_instruct, type = "upper", order = "hclust", 
                          tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.2))

plot_grid(p1, p2, ncol = 2, labels = c("Meta 70B", "Meta 70B Instruct"))
```


```{r}

### Generate ML3s sub DFM from SPACY ####
meta_llama_70_spacy <- read.table("hape_sample/spacy_data/spacy_Meta-Llama-3-70B.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)
anno_edit_ml70s <- structure(meta_llama_70_spacy, class = c("spacyr_parsed", "data.frame"))
subtkns_ml70s <- as.tokens(anno_edit_ml70s)
doc_categories_ml70s <- names(subtkns_ml70s) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))
docvars(subtkns_ml70s) <- doc_categories_ml70s
sub_dfm_ml70s <- subtkns_ml70s %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_ml70s, n = 10) |>
  select(1:3) |>
  gt() |>
  tab_header(
    title="Table 4: Highest Frequency Tokens in Meta Llama 70B"
  ) |>
  as_raw_html()
```
\newpage

```{r}

### Generate ML3s sub DFM from SPACY ####
meta_llama_70_Instruct_spacy <- read.table("hape_sample/spacy_data/spacy_Meta-Llama-3-70B-Instruct.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)
anno_edit_ml70s <- structure(meta_llama_70_spacy, class = c("spacyr_parsed", "data.frame"))
subtkns_ml70s <- as.tokens(anno_edit_ml70s)
doc_categories_ml70s <- names(subtkns_ml70s) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))
docvars(subtkns_ml70s) <- doc_categories_ml70s
sub_dfm_ml70s <- subtkns_ml70s %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_ml70s, n = 10) |>
  select(1:3) |>
  gt() |>
  tab_header(
    title="Table 4: Highest Frequency Tokens in Meta Llama 70B Instruct"
  ) |>
  as_raw_html()
```

```{r}
### Generate chunk sub DFM from SPACY ####
chunk_spacy <- read.table("hape_sample/spacy_data/spacy_chunk_2.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)
anno_edit_chunk <- structure(chunk_spacy, class = c("spacyr_parsed", "data.frame"))
subtkns_chunk <- as.tokens(anno_edit_chunk)
doc_categories_chunk <- names(subtkns_chunk) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))
docvars(subtkns_chunk) <- doc_categories_chunk
sub_dfm_chunk <- subtkns_chunk %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm()

textstat_frequency(sub_dfm_chunk, n = 10) |>
  select(1:3) |>
  gt() |>
  tab_header(
    title="Table 5: Highest Frequency Tokens in In Sample Human"
  ) |>
  as_raw_html()
```


```{r}
gpt_4o_spacy <- read.table("hape_sample/spacy_data/spacy_gpt-4o-2024-08-06.tsv", header=TRUE, sep='\t', quote="", fill=TRUE)
anno_edit_gpt4o <- structure(gpt_4o_spacy, class = c("spacyr_parsed", "data.frame"))
subtkns_gpt4o <- as.tokens(anno_edit_gpt4o)
doc_categories_gpt4o <- names(subtkns_gpt4o) %>%
  data.frame(text_type = .) %>%
  mutate(text_type = str_extract(text_type, "^[a-z]+"))

docvars(subtkns_gpt4o) <- doc_categories_gpt4o


sub_dfm_gpt4o <- subtkns_gpt4o %>%
  tokens_select("^.[a-zA-Z0-9]+.[a-z]", selection = "keep", valuetype = "regex", case_insensitive = T) %>%
  dfm() 

textstat_frequency(sub_dfm_gpt4o, n = 8) |>
  select(1:3) |>
  gt() |>
  tab_header(
    title="Table 6: Highest Frequency Tokens in GPT 4o"
  ) |>
  as_raw_html()

```
