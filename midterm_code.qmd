
---
title: "An Investigagtion into Model Preformance for Identification of Human vs. AI text"
subtitle: "A report to the Quantum Insight AI research team."
author: "Jenny, Lauren, Amalia"
date: last-modified
format:
  pdf:
    number-sections: true
    indent: true
    toc: false
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
execute:
  echo: false
---

# Statement of the Problem
Quantum Insights is aiming to distinguish human-authored text from AI-generated content. The team developed initial models that, on in-sample test data, correctly differentiated human-generated text from AI generated text with high accuracy ($81.78%-96.25%$). When this model was applied to out-of sample test data, however, the accuracy of the models drops significantly, with the lowest accuracy on the meta models being $52.25%$. Our team is interested in determining the contributing factors to this drop in performance and recommending potential next steps to solve this problem and improve the models' accuracy.

# Approach

The Team investigated this problem on three different avenues. We analyzed the models to determine what variables were of greatest importance when making classifications of texts. We then looked at the correlation matrices of biber features for each of the data sets for in and out of sample data to determine whether there were significantly different trends that might over specify classifications and hamper the models' generalizability. While investigating the correlation matrices, we also investigated the text files themselves, to see if there were any obvious trends or similarities among the human and machine samples in our data. We isolated and analyzed the prompts given to humans and LLMs for the out of sample data to determine whether prompt engineering could play a role in low performance. Finally, we analyzed the histograms of key features to further illuminate any underlying issues.


# Summary of Findings



# Recommendations



# Appendix

```{r, include=FALSE}
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
library(knitr)
library(gt)
library(syuzhet)
library(dplyr)
library(nFactors)
source("../R/helper_functions.R")
source("../R/keyness_functions.R")
source("../R/mda_functions.R")

```


```{r, include = FALSE}
####read in biber data ####
human_chunk_biber <- read_tsv("hape_sample/biber_data/biber_chunk_2.tsv")

gpt_4o_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-2024-08-06.tsv")

gpt_4o_mini_biber <- read_tsv("hape_sample/biber_data/biber_gpt-4o-mini-2024-07-18.tsv")

meta_llama_3_8b_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B.tsv")

meta_llama_3_8b_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-8B-Instruct.tsv")

meta_llama_3_70B_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B.tsv")

meta_llama_3_70B_Instruct_biber <- read_tsv("hape_sample/biber_data/biber_Meta-Llama-3-70B-Instruct.tsv")

arxiv_biber <- read_tsv("arxiv_sample/arxiv_biber.tsv")
```

```{r}

arxiv_biber_human <- arxiv_biber %>% 
  filter(str_detect(doc_id, "human")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

arxiv_biber_machine <- arxiv_biber %>% 
  filter(str_detect(doc_id, "machine")) %>%
  select(where(~ is.numeric(.x) && sum(.x) != 0))

# only filter arxiv out 

```

```{r, fig.cap="Correlation Matrix of Out-Sample Human", out.height = "30%"}
bc_cor <- cor(arxiv_biber_human[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of Out-Sample Machine", out.height = "30%"}
bc_cor <- cor(arxiv_biber_machine[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```

```{r, fig.cap="Correlation Matrix of In-Sample Human", out.height = "30%"}
bc_cor <- cor(human_chunk_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of In-Sample GPT 4o", out.height = "30%"}
bc_cor <- cor(gpt_4o_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
As an example of graph interpretation, the high positive correlation between biber features such as *f_09* and *f_59* and the high negative correlation between features such as *f_44* and *f_06* could indicate that the corpus has an informal register and is comprised primarily of informal texts. This can be seen due to the high usage of contractions, which are uncommon in formal texts. Furthermore, the negative correlation between mean word length and first person pronouns indicate that words are often shorter where first person pronouns occur - further bolstering the claim that this corpus has an informal register.


```{r, fig.cap="Correlation Matrix of In-Sample GPT 4o Mini", out.height = "30%"}
bc_cor <- cor(gpt_4o_mini_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of In-Sample Meta 8B", out.height = "30%"}
bc_cor <- cor(meta_llama_3_8b_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of In-Sample Meta 8B Instruct", out.height = "30%"}
bc_cor <- cor(meta_llama_3_8b_Instruct_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of In-Sample Meta 70B", out.height = "30%"}
bc_cor <- cor(meta_llama_3_70B_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```
```{r, fig.cap="Correlation Matrix of In-Sample Meta 70B Instruct", out.height = "30%"}
bc_cor <- cor(meta_llama_3_70B_Instruct_biber[-1], method = "pearson")
corrplot::corrplot(bc_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, diag = F, tl.cex = 0.2)
```


```{r}
screeplot_mda(human_chunk_biber)
```


```{r, include=FALSE}
#read in text cunks for analysis

text_chunk <- read_tsv("hape_sample/text_data/text_chunk_2.tsv")

```


General comments for interpreting the correlational plots -- delete later --

Positive Correlation means that as one linguistic feature increases, so too does the other

Negative correlation means that as one linguistic feature increases, the other decreases

Zero correlation means that there is no linear relationship between the features

As a general example for interpreting the graphs, if the feature nouns (might or might not be an actual feature, this is an example not an actual comment) and adjectives show a dark blue color, then the texts where nouns are frequent also tend to use more adjectives. So, when we extrapolate this to new texts or new tests, maybe we are expecting texts with a lot of nouns to have a lot of adjectives and thus might miss several test cases where this is not the case.

On the flip side, a dark red color would show a negative correlation which could mean???


what does a high negative correlation establish? high positive? why do some graphs have very minimal correlation overall. Note that this is for the entire data set --> if you have a lot of a similar writing style, how might this reflect in the graphs and what might this say about our models. As an example, if the models are highly trained to recognize a specific pattern of relationships, would they do well at then recognizing a different pattern? Are there patterns across the biber features?

I don't know how relevant this stuff is, but it is food for thought, so to speak.

For these interpretations, context is important. If you are going to comment on the relationship of a text set, might as well also look at the data itself to see if there are any visual patterns? Is there anything to suggest that the model is more used to formal/informal language, etc...
